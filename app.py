import gradio as gr
import os
import shutil
import subprocess
import sys
import cv2
from datetime import datetime

# ì–¼êµ´ ì¶”ì¶œ í•¨ìˆ˜ ì¶”ê°€
def extract_face_or_image(input_path, output_image_path):
    ext = os.path.splitext(input_path)[-1].lower()
    if ext in ['.jpg', '.jpeg', '.png']:
        shutil.copyfile(input_path, output_image_path)
        print(f"[INFO] ì´ë¯¸ì§€ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©: {output_image_path}")
        return

    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    vidcap = cv2.VideoCapture(input_path)
    found = False

    while True:
        success, frame = vidcap.read()
        if not success:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        if len(faces) > 0:
            cv2.imwrite(output_image_path, frame)
            print(f"[INFO] ì–¼êµ´ì´ ê²€ì¶œëœ í”„ë ˆì„ì„ {output_image_path} ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            found = True
            break

    vidcap.release()

    if not found:
        raise RuntimeError("ì˜ìƒì—ì„œ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# Wav2Lip ì‹¤í–‰ í•¨ìˆ˜
def run_wav2lip(face_path, audio_path, output_path):
    python_path = sys.executable
    print("[INFO] Running Wav2Lip...")
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = "2"
    result = subprocess.run(
        [python_path, "run_wav2lip.py",
         "--face", face_path,
         "--audio", audio_path,
         "--outfile", output_path],
        cwd="Wav2Lip",
        env=env
    )
    if result.returncode != 0:
        raise RuntimeError("Wav2Lip failed to run properly")

# HYFace ì‹¤í–‰ í•¨ìˆ˜ (ì´ì œ í•­ìƒ ì´ë¯¸ì§€ê°€ ë“¤ì–´ê°)
def run_hyface(target_image_path, source_audio_path, output_audio_path, output_vocal_path):
    print("[INFO] Running HYFace...")
    python_path = sys.executable
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = "1"
    result = subprocess.run(
        [python_path, "run_hyface.py",
         "--target", target_image_path,
         "--source", source_audio_path,
         "--output_path", output_audio_path,
         "--output_vocal_path", output_vocal_path],
        cwd="HYFace_real_final",
        env=env
    )
    if result.returncode != 0:
        raise RuntimeError("HYFace failed to run properly")

def replace_audio_in_video(video_path, new_audio_path, output_path):
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
    if not os.path.exists(new_audio_path):
        raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {new_audio_path}")

    command = [
        "ffmpeg", "-y",
        "-i", video_path,
        "-i", new_audio_path,
        "-c:v", "copy",
        "-map", "0:v:0",
        "-map", "1:a:0",
        "-shortest",
        output_path
    ]
    print(f"[INFO] FFmpeg ì‹¤í–‰: {' '.join(command)}")
    subprocess.run(command, check=True)
    return output_path

# ì „ì²´ íŒŒì´í”„ë¼ì¸
def inference(video_or_image_path, audio_path, final_output_path):
    hyface_output_path = final_output_path.replace(".mp4", "_hyface.wav")
    hyface_vocal_path = final_output_path.replace(".mp4", "_hyface_vocal.wav")
    wav2lip_path = final_output_path.replace(".mp4", "_wav2lip.mp4")

    # ì–¼êµ´ ì´ë¯¸ì§€ ì¶”ì¶œ (ì´ë¯¸ì§€/ì˜ìƒ êµ¬ë¶„ ìë™)
    face_image_path = final_output_path.replace(".mp4", "_face.jpg")
    extract_face_or_image(video_or_image_path, face_image_path)

    run_hyface(target_image_path=face_image_path,
               source_audio_path=audio_path,
               output_audio_path=hyface_output_path,
               output_vocal_path=hyface_vocal_path)

    run_wav2lip(face_path=video_or_image_path,  # ì˜ìƒ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                audio_path=hyface_vocal_path,
                output_path=wav2lip_path)

    replace_audio_in_video(wav2lip_path, hyface_output_path, final_output_path)

    for f in [hyface_output_path, hyface_vocal_path, wav2lip_path, face_image_path]:
        if os.path.exists(f):
            os.remove(f)

# Gradioì—ì„œ í˜¸ì¶œí•  process í•¨ìˆ˜
def process(video_file, audio_file):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    work_dir = os.path.join("/home/aikusrv03/lipsync/jobs", timestamp)
    os.makedirs(work_dir, exist_ok=True)

    video_or_image_path = os.path.join(work_dir, os.path.basename(video_file.name))
    audio_path = os.path.join(work_dir, os.path.basename(audio_file.name))

    shutil.copyfile(video_file.name, video_or_image_path)
    shutil.copyfile(audio_file.name, audio_path)

    output_path = os.path.join(work_dir, "final_output.mp4")
    inference(video_or_image_path, audio_path, output_path)
    return output_path

# Gradio UI
with gr.Blocks(
    css="""
    .gradio-container {
        text-align: center;
    }
    #main_column {
        align-items: center;
        justify-content: center;
    }
    """
) as demo:
    gr.Markdown("# ë‚´ê°€ ê°€ìˆ˜ê°€ ë  ìƒì¸ê°€ ğŸ¤")
    gr.Markdown("ë³¸ê²© ê´€ìƒìœ¼ë¡œë§Œ ë¦½ì‹±í¬ ì˜ìƒ ë§Œë“¤ê¸°")

    with gr.Column(elem_id="main_column"):
        video_input = gr.File(label="ì˜ìƒ or ì´ë¯¸ì§€ ì—…ë¡œë“œ", file_types=[".mp4", ".mov", ".avi", ".mkv", ".jpg", ".jpeg", ".png"])
        video_preview = gr.Video(label="ì—…ë¡œë“œ ë¯¸ë¦¬ë³´ê¸°")

        audio_input = gr.File(label="ìŒì•… íŒŒì¼ ì—…ë¡œë“œ", file_types=[".mp3", ".wav", ".aac"])
        audio_preview = gr.Audio(label="ì—…ë¡œë“œí•œ ì˜¤ë””ì˜¤ ë¯¸ë¦¬ë“£ê¸°")

        generate_btn = gr.Button("ğŸ¬ ì˜ìƒ ìƒì„±í•˜ê¸°")
        result_video = gr.Video(label="ğŸ§ ê²°ê³¼ ì˜ìƒ")

    video_input.change(fn=lambda f: f.name if f else None, inputs=video_input, outputs=video_preview)
    audio_input.change(fn=lambda f: f.name if f else None, inputs=audio_input, outputs=audio_preview)

    generate_btn.click(fn=process, inputs=[video_input, audio_input], outputs=result_video)

demo.queue()

if __name__ == "__main__":
    demo.launch(share=True)
